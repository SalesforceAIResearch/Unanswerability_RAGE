"""
This file is to verify the dataset generated by RAGAS is useful and good quality.
"""
import json
import re
from collections import defaultdict
from prompts_verify import verify_question_prompt
from langchain_openai.chat_models import ChatOpenAI
from ragas.llms import LangchainLLMWrapper
import asyncio
from ragas.llms.json_load import json_loader
from tqdm import tqdm

async def verify_question(question, answer, context, generator_llm_model):
    prompt = verify_question_prompt.format(question=question, answer=answer, context='\n'.join(context))
    results = await generator_llm_model.generate(prompt=prompt, is_async=False)
    output = await json_loader.safe_load(
        results.generations[0][0].text.strip(), llm=generator_llm_model, is_async=False
    )
    # print(prompt)
    # print(output)
    try:
        usefulness = output["usefulness"]
        correctness = output["correctness"]
        answerable = output["answerable"]
        if usefulness == "1" and correctness == "1" and answerable == "1":
            return True
    except:
        return False
async def verify_ragas_dataset():
    """
    Put your data file and reference file here
    :return: stats for citation
    """
    path_data = "/src/unanswerable_query/data/triviaQA/answerable.json"
    generator_llm = "gpt-4-turbo"
    generator_llm_model = LangchainLLMWrapper(ChatOpenAI(model_name=generator_llm))

    with open(path_data, "r") as f:
        dataset = json.load(f)

    new_dataset = defaultdict(list)
    for i in tqdm(range(len(dataset["question"]))):
        question = dataset["question"][i]
        context = dataset["contexts"][i]
        answer = dataset["ground_truth"][i]
        print('question', question)
        print('context', context)
        print('answer', answer)
        if await verify_question(question, answer, context, generator_llm_model):
            new_dataset["question"].append(question)
            new_dataset["contexts"].append(context)
            new_dataset["ground_truth"].append(answer)

    with open("/src/unanswerable_query/data/triviaQA/answerable_filter.json", "w") as f:
        json.dump(new_dataset, f)
    print(len(new_dataset["question"]))

async def verify_tf_dataset():
    path_data = "/data/trailhead/trailhead_true_false.json"
    generator_llm = "gpt-4-turbo"
    generator_llm_model = LangchainLLMWrapper(ChatOpenAI(model_name=generator_llm))

    with open(path_data, 'r') as json_file:
        json_list = list(json_file)

    new_dataset = defaultdict(list)
    for json_str in json_list:
        result = json.loads(json_str)
        question = result["question"]
        context = [result["context"]]
        answer = result["ground_truth"]
        if await verify_question(question, answer, context, generator_llm_model):
            new_dataset["question"].append(question)
            new_dataset["contexts"].append(context)
            new_dataset["ground_truth"].append(answer)

    with open("/data/trailhead/trailhead_true_false_filter.json", "w") as f:
        json.dump(new_dataset, f)
    print(len(new_dataset["question"]))

if __name__ == "__main__":
    asyncio.run(verify_ragas_dataset())
    # asyncio.run(verify_tf_dataset())
